# pointcept/datasets/wind_shear.py
import os
import glob
import numpy as np
import pandas as pd
from torch.utils.data import Dataset
import logging
from pointcept.utils.logger import get_root_logger
from scipy.spatial import KDTree
from pointcept.datasets.builder import DATASETS
# æ–°å¢ï¼šå¯¼å…¥æ„å»ºtransformçš„å‡½æ•°
from pointcept.datasets.transforms.builder import build_transform  # æ³¨æ„è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼ˆä½ çš„æ–‡ä»¶æ˜¯transformers/builder.pyï¼‰


# å…³é”®ï¼šæ·»åŠ æ³¨å†Œè£…é¥°å™¨ï¼Œè®©æ•°æ®é›†è¢«DATASETSæ³¨å†Œè¡¨è¯†åˆ«
@DATASETS.register_module()
class WindShearDataset(Dataset):
    def __init__(self, split='train', data_root="D:/model/wind_datas/csv_labels",
                 transform=None, k_neighbors=16, radius=0.5, min_points=50):  # æ–°å¢min_pointså‚æ•°
        super().__init__()
        self.split = split
        self.data_root = data_root
        self.transform = build_transform(transform)
        self.k_neighbors = k_neighbors
        self.radius = radius
        self.min_points = min_points  # æ–°å¢ï¼šåˆå§‹åŒ–min_pointså±æ€§
        self.data_list = self._get_data_list()

        logger = get_root_logger()
        logging.info(f"WindShearDataset {split} split: {len(self.data_list)} scenes")

    def _get_data_list(self):
        # æ ¹æ®æ—¥æœŸåˆ’åˆ†æ•°æ®é›†
        if self.split == 'train':
            dates = [f"202303{i:02d}" for i in range(1, 23)]
        elif self.split == 'val':
            dates = [f"202303{i:02d}" for i in range(23, 29)]
        elif self.split == 'test':
            dates = [f"202303{i:02d}" for i in range(29, 32)]
        else:
            raise ValueError(f"Invalid split: {self.split}")

        data_list = []
        for date in dates:
            date_path = os.path.join(self.data_root, date)
            if not os.path.exists(date_path):
                continue

            # æŸ¥æ‰¾æ‰€æœ‰datasæ–‡ä»¶å¤¹
            datas_dirs = glob.glob(os.path.join(date_path, "datas*"))
            for datas_dir in datas_dirs:
                # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
                csv_files = glob.glob(os.path.join(datas_dir, "*_labeled.csv"))
                data_list.extend(csv_files)

        return data_list

    def _compute_neighborhood_features(self, coord, beamaz, feat, label):
        """ä¿®æ­£ï¼šç»“åˆcoordå’Œbeamazè®¡ç®—é‚»åŸŸï¼Œç‰¹å¾ç»´åº¦æ‰©å±•ä¸º9ç»´"""
        """ç»“åˆcoordå®é™…è·¨åº¦çš„beamazå½’ä¸€åŒ–ï¼Œç¡®ä¿ä¸ç©ºé—´ç»´åº¦æƒé‡å‡è¡¡"""
        # 1. è®¡ç®—coordå„ç»´åº¦è·¨åº¦åŠå¹³å‡è·¨åº¦ï¼ˆåŸºäºä½ çš„å®é™…æ•°æ®ï¼šxâ‰ˆ1.17ä¸‡ã€yâ‰ˆ1.11ä¸‡ã€zâ‰ˆ420ï¼‰
        coord_spans = [
            coord[:, 0].max() - coord[:, 0].min(),  # xè·¨åº¦ï¼š~11713.2
            coord[:, 1].max() - coord[:, 1].min(),  # yè·¨åº¦ï¼š~11050.2
            coord[:, 2].max() - coord[:, 2].min()  # zè·¨åº¦ï¼š~420.6
        ]
        avg_coord_span = np.mean(coord_spans)  # è®¡ç®—ç»“æœâ‰ˆ7728
        #print(
        #    f"  ğŸ“ coordå„ç»´åº¦è·¨åº¦ï¼šx={coord_spans[0]:.1f}, y={coord_spans[1]:.1f}, z={coord_spans[2]:.1f}ï¼Œå¹³å‡è·¨åº¦â‰ˆ{avg_coord_span:.0f}")

        # 2. Beamazå½’ä¸€åŒ–ï¼šä½¿å…¶è·¨åº¦ä¸coordå¹³å‡è·¨åº¦åŒé‡çº§ï¼ˆæ ¸å¿ƒä¿®æ­£ï¼‰
        beamaz_original_span = 360.0  # BeamazåŸå§‹èŒƒå›´ï¼š0~360åº¦
        if avg_coord_span > 0:
            norm_ratio = beamaz_original_span / avg_coord_span  # â‰ˆ360/7728â‰ˆ0.0466
            beamaz_normalized = beamaz / norm_ratio  # å½’ä¸€åèŒƒå›´ï¼š0~360/0.0466â‰ˆ0~7725
        else:
            norm_ratio = 3.6  # æç«¯æƒ…å†µï¼šcoordæ— è·¨åº¦æ—¶ç”¨é»˜è®¤æ¯”ä¾‹
            beamaz_normalized = beamaz / norm_ratio
        #print(
        #    f"  ğŸ”„ Beamazå½’ä¸€åŒ–ï¼šæ¯”ä¾‹={norm_ratio:.4f}ï¼Œå½’ä¸€åèŒƒå›´â‰ˆ{beamaz_normalized.min():.0f}~{beamaz_normalized.max():.0f}")

        # 3. ç»„åˆâ€œcoord + å½’ä¸€åŒ–Beamazâ€æ„å»ºKDTreeï¼ˆæ­¤æ—¶å„ç»´åº¦æƒé‡å‡è¡¡ï¼‰
        spatial_features = np.hstack([coord, beamaz_normalized.reshape(-1, 1)])  # shape: (N, 4)

        # 4. åç»­é‚»åŸŸè®¡ç®—é€»è¾‘ï¼ˆä¸å˜ï¼Œä¿æŒ9ç»´ç‰¹å¾ï¼‰
        if len(spatial_features) < self.k_neighbors:
            mean_feat = np.zeros_like(feat)
            std_feat = np.zeros_like(feat)
            new_feat = np.concatenate([feat, mean_feat, std_feat], axis=1)
            return new_feat, label.copy()

        tree = KDTree(spatial_features)
        _, indices = tree.query(spatial_features, k=self.k_neighbors)

        new_feat = np.zeros((len(spatial_features), 9), dtype=np.float32)
        new_label = np.zeros(len(spatial_features), dtype=np.int64)

        for i in range(len(spatial_features)):
            neighbor_indices = indices[i]
            neighbor_feat = feat[neighbor_indices]
            mean_feat = np.mean(neighbor_feat, axis=0)
            std_feat = np.std(neighbor_feat, axis=0)
            feat_i = feat[i].squeeze()
            new_feat[i] = np.concatenate([feat_i, mean_feat, std_feat])

            # 2. ä¼˜åŒ–æ ‡ç­¾é€»è¾‘ï¼šé‚»åŸŸå†…é£åˆ‡å˜ç‚¹å æ¯”â‰¥0.3æ‰æ ‡1ï¼ˆé˜ˆå€¼å¯è°ƒæ•´ï¼‰
            neighbor_labels = label[neighbor_indices]
            shear_ratio = np.sum(neighbor_labels == 1) / len(neighbor_labels)  # è®¡ç®—é‚»åŸŸé£åˆ‡å˜å æ¯”
            new_label[i] = 1 if shear_ratio >= 0.3 else 0  # å æ¯”é˜ˆå€¼è®¾ä¸º0.3ï¼ˆå¯æ ¹æ®æ•°æ®è°ƒæ•´ï¼‰

        return new_feat, new_label

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        csv_path = self.data_list[idx]

        # è¯»å–CSVæ•°æ®
        data = pd.read_csv(csv_path)

        # æ–°å¢ï¼šæ‰“å°å½“å‰æ–‡ä»¶çš„åˆ—åï¼ˆåªåœ¨è°ƒè¯•æ—¶ç”¨ï¼Œä¹‹åå¯ä»¥åˆ é™¤ï¼‰
        logging.debug(f"\nCSVæ–‡ä»¶è·¯å¾„ï¼š{csv_path}")
        #print("åˆ—ååˆ—è¡¨ï¼š", data.columns.tolist())  # è¿™è¡Œæ˜¯å…³é”®

        # æå–åæ ‡ã€é£é€Ÿå’Œæ ‡ç­¾
        # æ³¨æ„ï¼šåˆ—åå¯èƒ½æœ‰ç©ºæ ¼ï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰ï¼Œè¿™é‡Œå°è¯•ä¸¤ç§å¯èƒ½
        # è¯»å–åæ ‡ï¼ˆx,y,zï¼‰
        try:
            coord = data[["x", "y", "z"]].values.astype(np.float32)
        except KeyError:
            coord = data[[" x", " y", " z"]].values.astype(np.float32)

        # è¯»å–ç‰¹å¾ï¼ˆu, v, beamazï¼‰- æ–°å¢beamaz
        try:
            u = data["u"].values.astype(np.float32)
            v = data["v"].values.astype(np.float32)
            beamaz = data["BeamAz"].values.astype(np.float32)  # æ–°å¢beamazè¯»å–
        except KeyError:
            u = data[" u"].values.astype(np.float32)
            v = data[" v"].values.astype(np.float32)
            beamaz = data["BeamAz"].values.astype(np.float32)  # å¤„ç†å¸¦ç©ºæ ¼åˆ—å

        # ç»„åˆåŸå§‹ç‰¹å¾ï¼ˆu, v, beamazï¼‰- ç»´åº¦ä»2å˜ä¸º3
        feat = np.column_stack([u, v, beamaz])

        # è¯»å–æ ‡ç­¾
        label = data["wind_shear_label"].values.astype(np.int64)

        # è®¡ç®—é‚»åŸŸç‰¹å¾ï¼ˆä¼ å…¥beamazå‚ä¸é‚»åŸŸè®¡ç®—ï¼‰
        feat, label = self._compute_neighborhood_features(coord, beamaz, feat, label)

        # æ„å»ºæ•°æ®å­—å…¸
        data_dict = {
            'coord': coord,
            'feat': feat,  # æ­¤æ—¶featä¸º9ç»´
            'label': label,
            'path': csv_path,
            'beamaz': beamaz  # ä¿ç•™åŸå§‹beamazä¾›è°ƒè¯•
        }

        # æ‰§è¡Œé‡‡æ ·ç­‰å˜æ¢åï¼Œæ·»åŠ ç‚¹æ•°æ ¡éªŒ
        data_dict = self.transform(data_dict)

        # æ–°å¢ï¼šæ£€æŸ¥é‡‡æ ·åç‚¹æ•°æ˜¯å¦æ»¡è¶³æœ€å°è¦æ±‚
        sampled_num = len(data_dict['coord'])
        if sampled_num < self.min_points:  # ç°åœ¨self.min_pointså·²å®šä¹‰
            # æ‰“å°è­¦å‘Šä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            import warnings
            warnings.warn(f"æ ·æœ¬{data_dict['path']}é‡‡æ ·åç‚¹æ•°({sampled_num})ä¸è¶³ï¼Œå·²è·³è¿‡")  # ä¿®æ­£ä¸ºdata_dict['path']
            return None  # è¿”å›Noneæ ‡è®°ä¸ºæ— æ•ˆæ ·æœ¬

        return data_dict
