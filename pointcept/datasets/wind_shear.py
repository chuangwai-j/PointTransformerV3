# pointcept/datasets/wind_shear.py
import os
import glob
import numpy as np
import pandas as pd
from torch.utils.data import Dataset
import logging
from pointcept.utils.logger import get_root_logger
from scipy.spatial import KDTree
from pointcept.datasets.builder import DATASETS
# æ–°å¢ï¼šå¯¼å…¥æ„å»ºtransformçš„å‡½æ•°
from pointcept.datasets.transforms.builder import build_transform  # æ³¨æ„è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼ˆä½ çš„æ–‡ä»¶æ˜¯transformers/builder.pyï¼‰


# å…³é”®ï¼šæ·»åŠ æ³¨å†Œè£…é¥°å™¨ï¼Œè®©æ•°æ®é›†è¢«DATASETSæ³¨å†Œè¡¨è¯†åˆ«
@DATASETS.register_module()
class WindShearDataset(Dataset):
    def __init__(self, split='train', data_root="D:/model/wind_datas/csv_labels",
                 transform=None, k_neighbors=16, radius=0.5, min_points=50,
                 filter_full_paths=None):  # æ–°å¢min_pointså‚æ•°
        super().__init__()
        self.split = split
        self.data_root = data_root
        self.transform = build_transform(transform)
        self.k_neighbors = k_neighbors
        self.radius = radius
        self.min_points = min_points  # æ–°å¢ï¼šåˆå§‹åŒ–min_pointså±æ€§
        self.data_list = self._get_data_list()
        self.filter_full_paths = filter_full_paths if filter_full_paths is not None else []  # å­˜å‚¨å®Œæ•´è·¯å¾„åˆ—è¡¨
        if self.filter_full_paths:
            logging.info(f"å°†è¿‡æ»¤{len(self.filter_full_paths)}ä¸ªä½ä»·å€¼æ ·æœ¬ï¼ˆå®Œæ•´è·¯å¾„ï¼‰")

        logger = get_root_logger()
        logging.info(f"WindShearDataset {split} split: {len(self.data_list)} scenes")

    def _get_data_list(self):
        # æ ¹æ®æ—¥æœŸåˆ’åˆ†æ•°æ®é›†
        if self.split == 'train':
            dates = [f"202303{i:02d}" for i in range(1, 23)]
        elif self.split == 'val':
            dates = [f"202303{i:02d}" for i in range(23, 29)]
        elif self.split == 'test':
            dates = [f"202303{i:02d}" for i in range(29, 32)]
        else:
            raise ValueError(f"Invalid split: {self.split}")

        data_list = []
        for date in dates:
            date_path = os.path.join(self.data_root, date)
            if not os.path.exists(date_path):
                continue

            # æŸ¥æ‰¾æ‰€æœ‰datasæ–‡ä»¶å¤¹
            datas_dirs = glob.glob(os.path.join(date_path, "datas*"))
            for datas_dir in datas_dirs:
                # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
                csv_files = glob.glob(os.path.join(datas_dir, "*_labeled.csv"))
                data_list.extend(csv_files)

        return data_list

    def _compute_neighborhood_features(self, coord, beamaz, feat, label):
        """ä¿®æ­£ï¼šç»“åˆcoordå’Œbeamazè®¡ç®—é‚»åŸŸï¼Œç‰¹å¾ç»´åº¦æ‰©å±•ä¸º9ç»´"""
        """ç»“åˆcoordå®é™…è·¨åº¦çš„beamazå½’ä¸€åŒ–ï¼Œç¡®ä¿ä¸ç©ºé—´ç»´åº¦æƒé‡å‡è¡¡"""
        # 1. è®¡ç®—coordå„ç»´åº¦è·¨åº¦åŠå¹³å‡è·¨åº¦ï¼ˆåŸºäºä½ çš„å®é™…æ•°æ®ï¼šxâ‰ˆ1.17ä¸‡ã€yâ‰ˆ1.11ä¸‡ã€zâ‰ˆ420ï¼‰
        coord_spans = [
            coord[:, 0].max() - coord[:, 0].min(),  # xè·¨åº¦ï¼š~11713.2
            coord[:, 1].max() - coord[:, 1].min(),  # yè·¨åº¦ï¼š~11050.2
            coord[:, 2].max() - coord[:, 2].min()  # zè·¨åº¦ï¼š~420.6
        ]
        avg_coord_span = np.mean(coord_spans)  # è®¡ç®—ç»“æœâ‰ˆ7728
        #print(
        #    f"  ğŸ“ coordå„ç»´åº¦è·¨åº¦ï¼šx={coord_spans[0]:.1f}, y={coord_spans[1]:.1f}, z={coord_spans[2]:.1f}ï¼Œå¹³å‡è·¨åº¦â‰ˆ{avg_coord_span:.0f}")

        # 2. Beamazå½’ä¸€åŒ–ï¼šä½¿å…¶è·¨åº¦ä¸coordå¹³å‡è·¨åº¦åŒé‡çº§ï¼ˆæ ¸å¿ƒä¿®æ­£ï¼‰
        beamaz_original_span = 360.0  # BeamazåŸå§‹èŒƒå›´ï¼š0~360åº¦
        if avg_coord_span > 0:
            norm_ratio = beamaz_original_span / avg_coord_span  # â‰ˆ360/7728â‰ˆ0.0466
            beamaz_normalized = beamaz / norm_ratio  # å½’ä¸€åèŒƒå›´ï¼š0~360/0.0466â‰ˆ0~7725
        else:
            norm_ratio = 3.6  # æç«¯æƒ…å†µï¼šcoordæ— è·¨åº¦æ—¶ç”¨é»˜è®¤æ¯”ä¾‹
            beamaz_normalized = beamaz / norm_ratio
        #print(
        #    f"  ğŸ”„ Beamazå½’ä¸€åŒ–ï¼šæ¯”ä¾‹={norm_ratio:.4f}ï¼Œå½’ä¸€åèŒƒå›´â‰ˆ{beamaz_normalized.min():.0f}~{beamaz_normalized.max():.0f}")

        # 3. ç»„åˆâ€œcoord + å½’ä¸€åŒ–Beamazâ€æ„å»ºKDTreeï¼ˆæ­¤æ—¶å„ç»´åº¦æƒé‡å‡è¡¡ï¼‰
        spatial_features = np.hstack([coord, beamaz_normalized.reshape(-1, 1)])  # shape: (N, 4)

        # 4. åç»­é‚»åŸŸè®¡ç®—é€»è¾‘ï¼ˆä¸å˜ï¼Œä¿æŒ9ç»´ç‰¹å¾ï¼‰
        if len(spatial_features) < self.k_neighbors:
            mean_feat = np.zeros_like(feat)
            std_feat = np.zeros_like(feat)
            new_feat = np.concatenate([feat, mean_feat, std_feat], axis=1)
            return new_feat, label.copy()

        tree = KDTree(spatial_features)
        _, indices = tree.query(spatial_features, k=self.k_neighbors)

        new_feat = np.zeros((len(spatial_features), 9), dtype=np.float32)
        new_label = np.zeros(len(spatial_features), dtype=np.int64)

        for i in range(len(spatial_features)):
            neighbor_indices = indices[i]
            neighbor_feat = feat[neighbor_indices]
            mean_feat = np.mean(neighbor_feat, axis=0)
            std_feat = np.std(neighbor_feat, axis=0)
            # æ–°å¢ï¼šå¤„ç†std=0çš„æƒ…å†µï¼ˆæ›¿æ¢ä¸º1e-6ï¼Œé¿å…é™¤ä»¥0ï¼‰
            std_feat = np.where(std_feat == 0, 1e-6, std_feat)
            feat_i = feat[i].squeeze()
            new_feat[i] = np.concatenate([feat_i, mean_feat, std_feat])

            # 2. ä¼˜åŒ–æ ‡ç­¾é€»è¾‘ï¼šé‚»åŸŸå†…é£åˆ‡å˜ç‚¹å æ¯”â‰¥0.3æ‰æ ‡1ï¼ˆé˜ˆå€¼å¯è°ƒæ•´ï¼‰
            #neighbor_labels = generate_label[neighbor_indices]
            #shear_ratio = np.sum(neighbor_labels == 1) / len(neighbor_labels)  # è®¡ç®—é‚»åŸŸé£åˆ‡å˜å æ¯”
            #new_label[i] = 1 if shear_ratio >= 0.3 else 0  # å æ¯”é˜ˆå€¼è®¾ä¸º0.3ï¼ˆå¯æ ¹æ®æ•°æ®è°ƒæ•´ï¼‰

            # æ–°å¤šåˆ†ç±»é€»è¾‘ï¼šå–é‚»åŸŸä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«ï¼ˆå¤šæ•°æŠ•ç¥¨ï¼‰
            neighbor_labels = label[neighbor_indices]  # é‚»åŸŸå†…æ‰€æœ‰ç‚¹çš„åŸå§‹æ ‡ç­¾ï¼ˆ0-4ï¼‰
            # ç»Ÿè®¡é‚»åŸŸä¸­æ¯ä¸ªç±»åˆ«çš„å‡ºç°æ¬¡æ•°
            counts = np.bincount(neighbor_labels, minlength=5)  # minlength=5ç¡®ä¿0-4ç±»éƒ½è¢«ç»Ÿè®¡
            # å–æ¬¡æ•°æœ€å¤šçš„ç±»åˆ«ä½œä¸ºå½“å‰ç‚¹çš„æ ‡ç­¾ï¼ˆè‹¥æœ‰å¹³å±€ï¼Œå–æœ€å°ç±»åˆ«ï¼‰
            most_common_label = np.argmax(counts)
            new_label[i] = most_common_label

        # æ–°å¢ï¼šé‚»åŸŸè®¡ç®—åæ£€æŸ¥æ˜¯å¦å¼•å…¥NaN/inf
        if np.isnan(feat).any() or np.isinf(feat).any():
            nan_mask = np.isnan(feat).any(axis=1) | np.isinf(feat).any(axis=1)
            feat = feat[~nan_mask]
            coord = coord[~nan_mask]
            label = label[~nan_mask]
            beamaz = beamaz[~nan_mask]
            logging.warning(f"é‚»åŸŸè®¡ç®—åè¿‡æ»¤äº†{nan_mask.sum()}ä¸ªå«NaN/infçš„ç‚¹")

        return new_feat, new_label

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx, warnings=None):
        csv_path = self.data_list[idx]
        try:
            # è¯»å–CSVæ•°æ®
            data = pd.read_csv(csv_path)
        except Exception as e:
            logging.error(f"è¯»å–æ ·æœ¬{csv_path}å¤±è´¥ï¼š{str(e)}ï¼Œå·²è·³è¿‡")
            return None  # è¯»å–å¤±è´¥ç›´æ¥è·³è¿‡

        # ğŸŒŸ æ ¸å¿ƒï¼šæŒ‰å®Œæ•´è·¯å¾„è¿‡æ»¤ï¼Œä»…è¿‡æ»¤ä½ç‚¹æ•°çš„é‚£ä¸ªæ ·æœ¬
        if csv_path in self.filter_full_paths:
            #logging.warning(f"æ ·æœ¬ {csv_path} å› è·¯å¾„åŒ¹é…è¢«è¿‡æ»¤ï¼ˆä½ç‚¹æ•°ï¼‰ï¼Œå·²è·³è¿‡åŠ è½½")
            return None

        # æå–åæ ‡ï¼ˆx,y,zï¼‰ï¼Œå¼ºåŒ–å¼‚å¸¸å¤„ç†
        try:
            coord = data[["x", "y", "z"]].values.astype(np.float32)
        except KeyError:
            coord = data[[" x", " y", " z"]].values.astype(np.float32)
        # ğŸŒŸ æ–°å¢1ï¼šæ£€æŸ¥åæ ‡æ˜¯å¦æœ‰NaN/infï¼ˆæºå¤´è¿‡æ»¤ï¼‰
        coord_nan = np.isnan(coord).any(axis=1)
        coord_inf = np.isinf(coord).any(axis=1)
        if np.any(coord_nan | coord_inf):
            valid_mask = ~(coord_nan | coord_inf)
            coord = coord[valid_mask]
            logging.warning(f"æ ·æœ¬{csv_path}åŸå§‹åæ ‡å«{len(coord) - valid_mask.sum()}ä¸ªNaN/infç‚¹ï¼Œå·²è¿‡æ»¤")

        # æå–ç‰¹å¾ï¼ˆu, v, beamazï¼‰
        try:
            u = data["u"].values.astype(np.float32)
            v = data["v"].values.astype(np.float32)
            beamaz = data["BeamAz"].values.astype(np.float32)
        except KeyError:
            u = data[" u"].values.astype(np.float32)
            v = data[" v"].values.astype(np.float32)
            beamaz = data["BeamAz"].values.astype(np.float32)
        # ğŸŒŸ æ–°å¢2ï¼šæ£€æŸ¥u/v/beamazæ˜¯å¦æœ‰NaN/inf
        feat_nan = np.isnan(u) | np.isnan(v) | np.isnan(beamaz)
        feat_inf = np.isinf(u) | np.isinf(v) | np.isinf(beamaz)
        if np.any(feat_nan | feat_inf):
            valid_mask = ~(feat_nan | feat_inf)
            u = u[valid_mask]
            v = v[valid_mask]
            beamaz = beamaz[valid_mask]
            coord = coord[valid_mask]  # åŒæ­¥è¿‡æ»¤åæ ‡
            logging.warning(f"æ ·æœ¬{csv_path}åŸå§‹ç‰¹å¾å«{len(u) - valid_mask.sum()}ä¸ªNaN/infç‚¹ï¼Œå·²è¿‡æ»¤")

        # ç»„åˆåŸå§‹ç‰¹å¾ï¼ˆu, v, beamazï¼‰- ç»´åº¦ä»2å˜ä¸º3
        feat = np.column_stack([u, v, beamaz])

        # è¯»å–æ ‡ç­¾å¹¶æ£€æŸ¥æœ‰æ•ˆæ€§
        try:
            label = data["label"].values.astype(np.int64)
        except KeyError:
            label = data[" label"].values.astype(np.int64)
        # è¿‡æ»¤æ— æ•ˆæ ‡ç­¾ï¼ˆ0-4å¤–ï¼‰å¹¶åŒæ­¥è¿‡æ»¤å…¶ä»–å­—æ®µ
        valid_label_mask = (label >= 0) & (label <= 4)
        if not np.all(valid_label_mask):
            invalid_count = len(label) - valid_label_mask.sum()
            label = label[valid_label_mask]
            feat = feat[valid_label_mask]
            coord = coord[valid_label_mask]
            beamaz = beamaz[valid_label_mask]
            logging.warning(f"æ ·æœ¬{csv_path}å«{invalid_count}ä¸ªæ— æ•ˆæ ‡ç­¾ï¼ˆé0-4ï¼‰ï¼Œå·²è¿‡æ»¤")

        # è‹¥è¿‡æ»¤åæ— æœ‰æ•ˆç‚¹ï¼Œç›´æ¥è·³è¿‡
        if len(coord) == 0:
            logging.warning(f"æ ·æœ¬{csv_path}è¿‡æ»¤åæ— æœ‰æ•ˆç‚¹ï¼Œå·²è·³è¿‡")
            return None

        # è®¡ç®—é‚»åŸŸç‰¹å¾ï¼ˆä¼ å…¥beamazå‚ä¸é‚»åŸŸè®¡ç®—ï¼‰
        feat, label = self._compute_neighborhood_features(coord, beamaz, feat, label)

        # æ„å»ºæ•°æ®å­—å…¸
        data_dict = {
            'coord': coord,
            'feat': feat,  # æ­¤æ—¶featä¸º9ç»´
            'generate_label': label,
            'path': csv_path,
            'beamaz': beamaz  # ä¿ç•™åŸå§‹beamazä¾›è°ƒè¯•
        }

        # æ‰§è¡Œé‡‡æ ·ç­‰å˜æ¢åï¼Œæ·»åŠ ç‚¹æ•°æ ¡éªŒ
        if self.transform is not None:
            try:
                data_dict = self.transform(data_dict)
            except Exception as e:
                logging.error(f"æ ·æœ¬{csv_path}å˜æ¢å¤±è´¥ï¼š{str(e)}ï¼Œå·²è·³è¿‡")
                return None

        # ğŸŒŸ å¼ºåˆ¶æ¢å¤pathï¼ˆé˜²æ­¢å˜æ¢ä¸­æ„å¤–ä¸¢å¤±ï¼‰
        data_dict['path'] = csv_path

        # æ–°å¢ï¼šæ£€æŸ¥é‡‡æ ·åç‚¹æ•°æ˜¯å¦æ»¡è¶³æœ€å°è¦æ±‚
        sampled_num = len(data_dict['coord'])
        if sampled_num < self.min_points:  # ç°åœ¨self.min_pointså·²å®šä¹‰
            logging.warning(f"æ ·æœ¬{data_dict['path']}é‡‡æ ·åç‚¹æ•°({sampled_num})ä¸è¶³ï¼Œå·²è·³è¿‡")  # ä¿®æ­£ä¸ºdata_dict['path']
            return None  # è¿”å›Noneæ ‡è®°ä¸ºæ— æ•ˆæ ·æœ¬

        # æœ€ç»ˆæ ¡éªŒï¼šç¡®ä¿æ‰€æœ‰å­—æ®µæ— NaN/inf
        final_nan = (np.isnan(data_dict['coord']).any()
                     | np.isnan(data_dict['feat']).any()
                     | np.isnan(data_dict['generate_label']).any())
        if final_nan:
            logging.error(f"æ ·æœ¬{csv_path}æœ€ç»ˆæ•°æ®å«NaNï¼Œå·²è·³è¿‡")
            return None

        return data_dict
