import os
import yaml
import numpy as np
import torch
import matplotlib.pyplot as plt
from pointcept.datasets import build_dataset, build_dataloader
from pointcept.datasets.transforms import build_transform
from torch.utils.data.sampler import RandomSampler, SequentialSampler

def load_config(config_path):
    """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
    assert os.path.exists(config_path), f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{config_path}"
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    # å…³é”®ï¼šLinuxç¯å¢ƒä¸‹ä¿®æ­£data_rootè·¯å¾„ï¼ˆåŸé…ç½®æ˜¯Windowsè·¯å¾„D:/...ï¼‰
    for split in ["train", "val", "test"]:
        if "data_root" in config["data"][split]:
            # ç¤ºä¾‹ï¼šå°†D:/model/wind_datas/ æ”¹ä¸º Linuxè·¯å¾„ï¼ˆéœ€æ ¹æ®ä½ çš„å®é™…è·¯å¾„è°ƒæ•´ï¼ï¼‰
            config["data"][split]["data_root"] = config["data"][split]["data_root"].replace(
                "D:/model/wind_datas/csv_labels",
                "/home/wai/PointTransformerV3/data/wind_datas/csv_labels"  # ä½ çš„Linuxæ•°æ®è·¯å¾„
            )
    return config


def verify_dataset_config(dataset, split, data_cfg):
    print(f"\nğŸ“‹ éªŒè¯[{split}]æ•°æ®é›†é…ç½®")
    print("-" * 50)

    assert dataset.split == split, f"splité…ç½®é”™è¯¯ï¼šé¢„æœŸ{split}ï¼Œå®é™…{dataset.split}"
    assert dataset.data_root == data_cfg["data_root"], f"data_rooté…ç½®é”™è¯¯"
    assert dataset.k_neighbors == data_cfg["k_neighbors"], f"k_neighborsé…ç½®é”™è¯¯"
    print("âœ… åŸºç¡€å‚æ•°ï¼ˆsplit/data_root/k_neighborsï¼‰é…ç½®æ­£ç¡®")

    # éªŒè¯transformï¼ˆå«beamazå½’ä¸€åŒ–ï¼‰
    transform_class_names = [t.__class__.__name__ for t in dataset.transform.transforms]
    config_transform_types = [t["type"] for t in data_cfg["transform"]]
    assert transform_class_names == config_transform_types, \
        f"transformé¡ºåºæˆ–åç§°é”™è¯¯ï¼šé¢„æœŸ{config_transform_types}ï¼Œå®é™…{transform_class_names}"
    print(f"âœ… Transformé…ç½®æ­£ç¡®ï¼š{transform_class_names}")

    # éªŒè¯NormalizeWindï¼ˆå«beamazå‚æ•°ï¼‰
    normalize_tf = next(t for t in dataset.transform.transforms if t.__class__.__name__ == "NormalizeWind")
    normalize_cfg = next(t for t in data_cfg["transform"] if t["type"] == "NormalizeWind")
    assert normalize_tf.u_mean == normalize_cfg["u_mean"], f"u_meané…ç½®é”™è¯¯"
    assert normalize_tf.u_std == normalize_cfg["u_std"], f"u_stdé…ç½®é”™è¯¯"
    assert normalize_tf.v_mean == normalize_cfg["v_mean"], f"v_meané…ç½®é”™è¯¯"
    assert normalize_tf.v_std == normalize_cfg["v_std"], f"v_stdé…ç½®é”™è¯¯"
    assert normalize_tf.beamaz_mean == normalize_cfg["beamaz_mean"], f"beamaz_meané…ç½®é”™è¯¯"  # æ–°å¢
    assert normalize_tf.beamaz_std == normalize_cfg["beamaz_std"], f"beamaz_stdé…ç½®é”™è¯¯"      # æ–°å¢
    print("âœ… é£é€Ÿ+beamazå½’ä¸€åŒ–å‚æ•°é…ç½®æ­£ç¡®")

    # éªŒè¯GridSample
    grid_tf = next(t for t in dataset.transform.transforms if t.__class__.__name__ in ["GridSample", "WindShearGridSample"])
    grid_cfg = next(t for t in data_cfg["transform"] if t["type"] in ["GridSample", "WindShearGridSample"])
    assert grid_tf.grid_size == grid_cfg["grid_size"], f"grid_sizeé…ç½®é”™è¯¯"
    print("âœ… ç½‘æ ¼é‡‡æ ·å‚æ•°é…ç½®æ­£ç¡®")


def verify_data_compatibility(data_dict, model_cfg, csv_path):
    print("\nğŸ“ éªŒè¯æ•°æ®ä¸æ¨¡å‹å…¼å®¹æ€§")
    print("-" * 50)

    # ç‰¹å¾ç»´åº¦ä»6å˜ä¸º9ï¼ˆ3åŸå§‹+3å‡å€¼+3æ ‡å‡†å·®ï¼‰
    feat = data_dict["feat"]
    expected_in_channels = model_cfg["in_channels"]  # éœ€åœ¨é…ç½®ä¸­è®¾ä¸º9
    feat_np = feat.numpy() if torch.is_tensor(feat) else feat
    assert feat_np.shape[1] == expected_in_channels, \
        f"ç‰¹å¾ç»´åº¦ä¸æ¨¡å‹ä¸åŒ¹é…ï¼šæ¨¡å‹æœŸæœ›{expected_in_channels}ç»´ï¼Œå®é™…{feat_np.shape[1]}ç»´"
    print(f"âœ… ç‰¹å¾ç»´åº¦æ­£ç¡®ï¼š{feat_np.shape[1]}ç»´ï¼ˆä¸æ¨¡å‹in_channelsä¸€è‡´ï¼‰")

    # æ ‡ç­¾éªŒè¯
    label = data_dict["label"]
    expected_num_classes = model_cfg["num_classes"]
    label_np = label.numpy() if torch.is_tensor(label) else label
    assert np.all(np.isin(label_np, range(expected_num_classes))), \
        f"æ ‡ç­¾ç±»åˆ«ä¸æ¨¡å‹ä¸åŒ¹é…ï¼šæ¨¡å‹æœŸæœ›{expected_num_classes}ç±»ï¼Œå®é™…æ ‡ç­¾åŒ…å«{np.unique(label_np)}"

    # æ–°å¢ï¼šæ‰“å°å…¨1æ ‡ç­¾çš„æ ·æœ¬è·¯å¾„
    label_np = label.numpy() if torch.is_tensor(label) else label
    if np.all(label_np == 1):
        print(f"âš ï¸ è­¦å‘Šï¼šå½“å‰æ ·æœ¬å…¨ä¸ºé£åˆ‡å˜ç‚¹ï¼è·¯å¾„ï¼š{csv_path}")

    shear_ratio = np.sum(label_np == 1) / len(label_np)
    print(f"âœ… æ ‡ç­¾ç±»åˆ«æ­£ç¡®ï¼š{expected_num_classes}ç±»ï¼Œé£åˆ‡å˜ç‚¹å æ¯”={shear_ratio:.3f}")


def verify_dataloader_config(dataloader, train_cfg, split):
    print(f"\nğŸ”„ éªŒè¯[{split}]DataLoaderé…ç½®")
    print("-" * 50)

    assert dataloader.batch_size == train_cfg["batch_size"], f"batch_sizeé…ç½®é”™è¯¯"
    assert dataloader.num_workers == train_cfg["num_workers"], f"num_workersé…ç½®é”™è¯¯"

    if split == "train":
        assert isinstance(dataloader.sampler, RandomSampler), "è®­ç»ƒé›†æœªå¼€å¯shuffleï¼"
    else:
        assert isinstance(dataloader.sampler, SequentialSampler), f"{split}é›†ä¸åº”å¼€å¯shuffleï¼"
    print("âœ… DataLoaderå‚æ•°ï¼ˆbatch_size/num_workers/shuffleï¼‰é…ç½®æ­£ç¡®")


def visualize_config_impact(raw_data, processed_data, save_path="config_impact.png"):
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    axes[0].scatter(raw_data["coord"][:, 0], raw_data["coord"][:, 1],
                    c=raw_data["label"], cmap="coolwarm", s=10, alpha=0.6)
    axes[0].set_title(f"åŸå§‹æ•°æ®ï¼ˆæ— Transformï¼‰\nç‚¹æ•°ï¼š{len(raw_data['coord'])}")
    axes[0].set_xlabel("X")
    axes[0].set_ylabel("Y")

    axes[1].scatter(processed_data["coord"][:, 0], processed_data["coord"][:, 1],
                    c=processed_data["label"], cmap="coolwarm", s=10, alpha=0.6)
    axes[1].set_title("Processed Data (GridSample Applied)")
    axes[1].set_xlabel("X")
    axes[1].set_ylabel("Y")

    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"\nğŸ“Š é…ç½®æ•ˆæœå¯è§†åŒ–å·²ä¿å­˜ï¼š{save_path}ï¼ˆå·¦ï¼šåŸå§‹æ•°æ®ï¼Œå³ï¼šå¤„ç†åæ•°æ®ï¼‰")


if __name__ == "__main__":
    CONFIG_PATH = "configs/wind_shear/pointtransformer_v3.yaml"
    config = load_config(CONFIG_PATH)
    print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶ï¼š{CONFIG_PATH}")
    print("=" * 70)

    model_cfg = config["model"]
    print(f"ğŸ¯ æ¨¡å‹é…ç½®ï¼šin_channels={model_cfg['in_channels']}, num_classes={model_cfg['num_classes']}")

    for split in ["train", "val", "test"]:
        print("\n" + "=" * 70)
        print(f"===== å¼€å§‹éªŒè¯[{split}]é›†é…ç½® =====")

        data_cfg = config["data"][split]
        transform = build_transform({
            "type": "Compose",
            "transforms": data_cfg["transform"]
        })
        dataset_cfg = data_cfg.copy()
        dataset_cfg["transform"] = transform
        dataset = build_dataset(dataset_cfg)

        assert len(dataset) > 0, f"{split}é›†æ— æ•°æ®ï¼è¯·æ£€æŸ¥è·¯å¾„å’Œæ—¥æœŸèŒƒå›´"
        print(f"âœ… {split}é›†æ•°æ®é‡ï¼š{len(dataset)}ä¸ªCSVæ ·æœ¬")

        verify_dataset_config(dataset, split, data_cfg)

        data_dict = dataset[0]
        required_keys = ["coord", "feat", "label", "path", "beamaz"]  # æ–°å¢beamazæ£€æŸ¥
        for key in required_keys:
            assert key in data_dict, f"æ•°æ®å­—å…¸ç¼ºå°‘å…³é”®å­—æ®µï¼š{key}"
        print("âœ… æ•°æ®å­—å…¸å­—æ®µå®Œæ•´ï¼ˆå«æ–°å¢beamazï¼‰")

        verify_data_compatibility(data_dict, model_cfg, data_dict["path"])

        dataloader = build_dataloader(
            dataset=dataset,
            batch_size=config["train"]["batch_size"],
            num_workers=config["train"]["num_workers"],
            shuffle=(split == "train")
        )
        verify_dataloader_config(dataloader, config["train"], split)

        batch_data = next(iter(dataloader))
        assert batch_data["coord"].shape[0] == batch_data["feat"].shape[0] == batch_data["label"].shape[0], \
            "æ‰¹é‡æ•°æ®æ‹¼æ¥é”™è¯¯ï¼šcoord/feat/labelç‚¹æ•°ä¸åŒ¹é…"
        print("âœ… æ‰¹é‡æ•°æ®å¤„ç†æ­£ç¡®")

    print("\n" + "=" * 70)
    print("===== å¯è§†åŒ–é…ç½®æ•ˆæœ =====")
    raw_data_cfg = config["data"]["train"].copy()
    raw_data_cfg["transform"] = None
    raw_dataset = build_dataset(raw_data_cfg)
    visualize_config_impact(raw_dataset[2], dataset[2])

    print("\n" + "=" * 70)
    print("ğŸ‰ æ‰€æœ‰éªŒè¯æ­¥éª¤å®Œæˆï¼æ•°æ®åŠ è½½æ¨¡å—æ­£å¸¸ï¼Œå¯è¿›å…¥æ¨¡å‹è®­ç»ƒé˜¶æ®µ")
    print("=" * 70)