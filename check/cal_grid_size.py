import os
import glob
import numpy as np
import pandas as pd
from tqdm import tqdm
import logging

# --- 1. é…ç½®æ‚¨çš„è·¯å¾„å’Œå‚æ•° ---
# (ä¸ä¸Šä¸€ä¸ªè„šæœ¬ä¿æŒä¸€è‡´)

# æ‚¨çš„æ•°æ®æ ¹ç›®å½•
DATA_ROOT = "/mnt/d/model/wind_datas/csv_labels"

# è®­ç»ƒé›†å¯¹åº”çš„æ—¥æœŸæ–‡ä»¶å¤¹
TRAIN_DATES = [f"202303{i:02d}" for i in range(1, 23)]

# éœ€è¦è¿‡æ»¤çš„ä½ç‚¹æ•°æ ·æœ¬çš„å®Œæ•´è·¯å¾„
FILTER_PATHS_LIST = [
    "/mnt/d/model/wind_datas/csv_labels/20230310/datas4/period107_labeled.csv",
    "/mnt/d/model/wind_datas/csv_labels/20230319/datas1/nn217_labeled.csv",
    "/mnt/d/model/wind_datas/csv_labels/20230314/datas1/aa217_labeled.csv",
    "/mnt/d/model/wind_datas/csv_labels/20230317/datas1/gg1_labeled.csv",
    "/mnt/d/model/wind_datas/csv_labels/20230308/datas1/i1_labeled.csv",
    "/mnt/d/model/wind_datas/csv_labels/20230310/datas1/period110_labeled.csv"
]
FILTER_PATHS_SET = set(FILTER_PATHS_LIST)

# --- ğŸŒŸ ä¿®æ”¹1ï¼šæ·»åŠ æœ€å°é«˜åº¦é™åˆ¶ ---
MAX_HEIGHT = 1000.0
MIN_HEIGHT = 0.0  # å‡è®¾åœ°é¢ä¸º 0 ç±³


# --- 2. è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå®‰å…¨è¯»å–åˆ— ---

def get_columns(df, columns):
    """
    å°è¯•è¯»å–åˆ—ï¼Œå…¼å®¹å¸¦ç©ºæ ¼å’Œä¸å¸¦ç©ºæ ¼çš„åˆ—å
    """
    data = {}
    for col in columns:
        try:
            data[col] = df[col].values
        except KeyError:
            try:
                data[col] = df[" " + col].values
            except Exception as e:
                logging.error(f"æ— æ³•è¯»å–åˆ— {col} æˆ– ' {col}'. é”™è¯¯: {e}")
                raise e
    return data


# --- 3. ä¸»è®¡ç®—é€»è¾‘ ---

def analyze_data_for_grid_size():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    data_list = []
    logging.info("å¼€å§‹æœå¯»è®­ç»ƒé›†æ–‡ä»¶...")
    for date in TRAIN_DATES:
        date_path = os.path.join(DATA_ROOT, date)
        if not os.path.exists(date_path):
            continue
        datas_dirs = glob.glob(os.path.join(date_path, "datas*"))
        for datas_dir in datas_dirs:
            csv_files = glob.glob(os.path.join(datas_dir, "*_labeled.csv"))
            data_list.extend(csv_files)

    logging.info(f"å…±æ‰¾åˆ° {len(data_list)} ä¸ªè®­ç»ƒé›†æ–‡ä»¶ã€‚")

    # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„ç»Ÿè®¡æ•°æ®
    all_point_counts = []
    all_x_ranges = []
    all_y_ranges = []
    all_z_ranges = []

    pbar = tqdm(data_list, desc="åˆ†ææ–‡ä»¶")
    for csv_path in pbar:
        # 3.1 è¿‡æ»¤ä½ç‚¹æ•°æ ·æœ¬
        if csv_path in FILTER_PATHS_SET:
            continue

        try:
            # 3.2 è¯»å–æ•°æ® (åªè¯»åæ ‡)
            # ä¿®å¤ï¼šç¡®ä¿è¯»å–äº† label åˆ—ç”¨äºåç»­è¿‡æ»¤
            data = pd.read_csv(csv_path, usecols=['x', 'y', 'z', 'label'])
            if data.empty:
                continue

            # 3.3 æå–åæ ‡
            cols = get_columns(data, ['x', 'y', 'z', 'label'])
            coord = np.stack([cols['x'], cols['y'], cols['z']], axis=1)
            label = cols['label']

            # 3.4 æ¸…æ´— NaN/Inf (åŒ __getitem__)
            coord_nan = np.isnan(coord).any(axis=1)
            coord_inf = np.isinf(coord).any(axis=1)
            label_valid = (label >= 0) & (label <= 4)
            valid_mask = ~(coord_nan | coord_inf) & label_valid

            coord = coord[valid_mask]

            # 3.5 ğŸŒŸ ä¿®æ”¹2ï¼šåº”ç”¨é«˜åº¦è¿‡æ»¤ (0 <= z <= 1000)
            height_mask = (coord[:, 2] <= MAX_HEIGHT) & (coord[:, 2] >= MIN_HEIGHT)
            coord = coord[height_mask]

            if coord.shape[0] == 0:
                logging.warning(f"æ–‡ä»¶ {csv_path} è¿‡æ»¤åæ— æ•°æ®ã€‚")
                continue

            # 3.6 è®°å½•ç»Ÿè®¡æ•°æ®
            all_point_counts.append(coord.shape[0])
            # ä¿®å¤ï¼šç¡®ä¿åœ¨è®¡ç®—èŒƒå›´å‰ï¼Œç‚¹æ•°å¤§äº0
            if coord.shape[0] > 0:
                all_x_ranges.append(coord[:, 0].max() - coord[:, 0].min())
                all_y_ranges.append(coord[:, 1].max() - coord[:, 1].min())
                all_z_ranges.append(coord[:, 2].max() - coord[:, 2].min())

        except Exception as e:
            logging.error(f"å¤„ç†æ–‡ä»¶ {csv_path} å¤±è´¥: {e}", exc_info=True)

    logging.info("æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæ¯•ï¼Œå¼€å§‹è®¡ç®—ç»Ÿè®¡æ•°æ®...")

    # 4. è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    avg_point_count = np.mean(all_point_counts)
    min_point_count = np.min(all_point_counts)
    max_point_count = np.max(all_point_counts)

    avg_x_range = np.mean(all_x_ranges)
    avg_y_range = np.mean(all_y_ranges)
    avg_z_range = np.mean(all_z_ranges)

    # 5. æ‰“å°æŠ¥å‘Š
    print(f"\n--- ( {MIN_HEIGHT}m <= z <= {MAX_HEIGHT}m ) è®­ç»ƒæ•°æ®åˆ†ææŠ¥å‘Š ---")
    print("\n[ç‚¹æ•°ç»Ÿè®¡ (è¿‡æ»¤å, é‡‡æ ·å‰)]")
    print(f"  å¹³å‡ç‚¹æ•°: {avg_point_count:.0f} (æ¯ä¸ªæ ·æœ¬)")
    print(f"  ç‚¹æ•°èŒƒå›´: {min_point_count:.0f} (æœ€å°‘) - {max_point_count:.0f} (æœ€å¤š)")

    print("\n[ç©ºé—´èŒƒå›´ç»Ÿè®¡ (å¹³å‡å€¼)]")
    print(f"  å¹³å‡ X è½´èŒƒå›´: {avg_x_range:.1f} (ç±³)")
    print(f"  å¹³å‡ Y è½´èŒƒå›´: {avg_y_range:.1f} (ç±³)")
    print(f"  å¹³å‡ Z è½´èŒƒå›´: {avg_z_range:.1f} (ç±³)  (æœ€å¤§ä¸º {MAX_HEIGHT})")

    print("\n--- å¦‚ä½•é€‰æ‹©æ–°çš„ grid_size ---")
    print("`grid_size` æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæ‚¨éœ€è¦æ ¹æ®ä»¥ä¸Šç»Ÿè®¡æ•°æ®è¿›è¡Œæƒè¡¡ã€‚")
    print("ç›®æ ‡ï¼šé€‰æ‹© (grid_x, grid_y, grid_z)ï¼Œä½¿é‡‡æ ·åçš„ç‚¹æ•°åœ¨åˆç†èŒƒå›´ï¼ˆå¦‚ 1000 - 5000 ç‚¹ï¼‰ã€‚")
    print("\n[æ¨èçš„è®¾ç½®ç­–ç•¥ (é€‰æ‹©ä¸€ç§)]")

    # ç­–ç•¥1: ç›®æ ‡ X/Y è½´ 100ä¸ªä½“ç´ , Z è½´ 50ä¸ªä½“ç´  (åˆ†è¾¨ç‡ä¸­ç­‰)
    rec_x_mid = avg_x_range / 100
    rec_y_mid = avg_y_range / 100
    rec_z_mid = avg_z_range / 50
    print("\n[é€‰é¡¹1: ä¸­ç­‰åˆ†è¾¨ç‡ (æ¨èèµ·ç‚¹)]")
    print("  ç›®æ ‡: X/Y è½´çº¦ 100 ä¸ªä½“ç´ , Z è½´çº¦ 50 ä¸ªä½“ç´ ")
    # æ·»åŠ ä¿æŠ¤ï¼Œé˜²æ­¢ avg_z_range ä¸º 0
    print(f"  - grid_size: [{rec_x_mid:.1f}, {rec_y_mid:.1f}, {max(0.1, rec_z_mid):.1f}]")
    print(f"  (è®¡ç®—: X={avg_x_range:.0f}/100, Y={avg_y_range:.0f}/100, Z={avg_z_range:.0f}/50)")

    # ç­–ç•¥2: ç›®æ ‡ X/Y è½´ 150ä¸ªä½“ç´ , Z è½´ 75ä¸ªä½“ç´  (åˆ†è¾¨ç‡è¾ƒé«˜)
    rec_x_high = avg_x_range / 150
    rec_y_high = avg_y_range / 150
    rec_z_high = avg_z_range / 75
    print("\n[é€‰é¡¹2: è¾ƒé«˜åˆ†è¾¨ç‡ (ç‚¹æ•°æ›´å¤š, æ˜¾å­˜å ç”¨é«˜)]")
    print("  ç›®æ ‡: X/Y è½´çº¦ 150 ä¸ªä½“ç´ , Z è½´çº¦ 75 ä¸ªä½“ç´ ")
    print(f"  - grid_size: [{rec_x_high:.1f}, {rec_y_high:.1f}, {max(0.1, rec_z_high):.1f}]")
    print(f"  (è®¡ç®—: X={avg_x_range:.0f}/150, Y={avg_y_range:.0f}/150, Z={avg_z_range:.0f}/75)")

    # ç­–ç•¥3: ç›®æ ‡ X/Y è½´ 80ä¸ªä½“ç´ , Z è½´ 40ä¸ªä½“ç´  (åˆ†è¾¨ç‡è¾ƒä½)
    rec_x_low = avg_x_range / 80
    rec_y_low = avg_y_range / 80
    rec_z_low = avg_z_range / 40
    print("\n[é€‰é¡¹3: è¾ƒä½åˆ†è¾¨ç‡ (ç‚¹æ•°æ›´å°‘, é€Ÿåº¦å¿«)]")
    print("  ç›®æ ‡: X/Y è½´çº¦ 80 ä¸ªä½“ç´ , Z è½´çº¦ 40 ä¸ªä½“ç´ ")
    print(f"  - grid_size: [{rec_x_low:.1f}, {rec_y_low:.1f}, {max(0.1, rec_z_low):.1f}]")
    print(f"  (è®¡ç®—: X={avg_x_range:.0f}/80, Y={avg_y_range:.0f}/80, Z={avg_z_range:.0f}/40)")

    print("\n[é‡è¦æç¤º]")
    print(f"1. æ‚¨å½“å‰çš„ `grid_size` æ˜¯ [122.6, 118.0, 5.4]ã€‚è¯·å¯¹æ¯”ä¸€ä¸‹ 'é€‰é¡¹1' çš„æ¨èå€¼ã€‚")
    print(f"2. é€‰å®š `grid_size` åï¼Œè¯·åœ¨è®­ç»ƒæ—¶å¯†åˆ‡å…³æ³¨ `collate_fn` æ‰“å°çš„æ—¥å¿—ã€‚")
    print(f"3. æŸ¥æ‰¾ `Batchç”ŸæˆæˆåŠŸï¼š... æ€»ç‚¹æ•°=XXXXX` è¿™æ¡æ—¥å¿—ã€‚")
    print(f"4. å¦‚æœæ€»ç‚¹æ•°ï¼ˆè¡¥ç‚¹åï¼‰ç»å¸¸æ˜¯ 1536, 1920 (å³ 384*4 æˆ– 384*5)ï¼Œè¯´æ˜é‡‡æ ·åç‚¹æ•°è¾ƒå°‘ã€‚")
    print(f"5. å¦‚æœæ€»ç‚¹æ•°éå¸¸å¤§ (å¦‚ > 10000)ï¼Œè¯´æ˜é‡‡æ ·ç‚¹è¿‡å¤šï¼Œæ‚¨å¯èƒ½éœ€è¦å¢å¤§ `grid_size` (ä½¿ç”¨ 'é€‰é¡¹3' çš„æ€è·¯)ã€‚")


if __name__ == "__main__":
    analyze_data_for_grid_size()