import os
import sys
import logging

# è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜ï¼šå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonæœç´¢è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import yaml
import torch
import numpy as np
from datetime import datetime
from tqdm import tqdm
#from sklearn.metrics import recall_score, precision_score, f1_score
import warnings
import matplotlib.pyplot as plt

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from pointcept.datasets.builder import build_train_dataloader, build_val_dataloader
from pointcept.models import build_model
from pointcept.utils.logger import get_logger
from pointcept.utils.checkpoint import save_checkpoint
from pointcept.utils.logging import setup_logging  # å¯¼å…¥å·¥å…·å‡½æ•°

# 1. é…ç½®å…¨å±€æ—¥å¿—ï¼ˆåªè°ƒç”¨1æ¬¡ï¼ï¼‰
logger = setup_logging(log_dir="./logs")  # æ—¥å¿—æ–‡ä»¶å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„logsæ–‡ä»¶å¤¹


# -------------------------- æ–°å¢ï¼šç»Ÿè®¡è®­ç»ƒé›†ç±»åˆ«æ•°é‡ï¼ˆç‚¹çº§åˆ«ï¼‰ --------------------------
def count_train_classes(train_loader):
    """
    ç»Ÿè®¡è®­ç»ƒé›†æ¯ä¸ªç±»åˆ«çš„ç‚¹çº§åˆ«æ ·æœ¬æ•°é‡
    è¿”å›ï¼šç±»åˆ«æ•°é‡å­—å…¸ {0: æ•°é‡, 1: æ•°é‡, ..., 4: æ•°é‡}
    """
    class_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0}  # å›ºå®š5ä¸ªç±»åˆ«
    logger.info("å¼€å§‹ç»Ÿè®¡è®­ç»ƒé›†ç±»åˆ«æ•°é‡ï¼ˆç‚¹çº§åˆ«ï¼‰...")

    for batch_idx, batch in enumerate(tqdm(train_loader, desc="ç»Ÿè®¡è®­ç»ƒé›†ç±»åˆ«")):
        # è·³è¿‡ç©ºbatch
        if batch is None or len(batch.get('path', [])) == 0:
            logger.warning(f"ç»Ÿè®¡æ—¶è·³è¿‡ç©ºbatch {batch_idx}")
            continue

        # è·å–å½“å‰batchçš„æ ‡ç­¾ï¼ˆç‚¹çº§åˆ«ï¼Œå­—æ®µä¸ºgenerate_labelï¼‰
        labels = batch['generate_label'].cpu().numpy()  # è½¬ç§»åˆ°CPUé¿å…è®¾å¤‡å ç”¨
        # ç»Ÿè®¡å½“å‰batchå„ç±»åˆ«æ•°é‡
        batch_counts = np.bincount(labels, minlength=5)  # ç¡®ä¿è¿”å›5ä¸ªç±»åˆ«ï¼ˆ0-4ï¼‰

        # ç´¯åŠ è‡³æ€»ç»Ÿè®¡
        for cls in range(5):
            class_counts[cls] += batch_counts[cls]

    # è®¡ç®—å„ç±»åˆ«å æ¯”
    total_points = sum(class_counts.values())
    logger.info("\n" + "=" * 60)
    logger.info("è®­ç»ƒé›†ç±»åˆ«æ•°é‡ç»Ÿè®¡ç»“æœï¼ˆç‚¹çº§åˆ«ï¼‰")
    logger.info("=" * 60)
    for cls, count in class_counts.items():
        ratio = (count / total_points) * 100 if total_points > 0 else 0
        if cls == 0:
            cls_name = "æ— é£åˆ‡å˜"
        elif cls == 1:
            cls_name = "è½»å¾®é£åˆ‡å˜"
        elif cls == 2:
            cls_name = "ä¸­åº¦é£åˆ‡å˜"
        elif cls == 3:
            cls_name = "é‡åº¦é£åˆ‡å˜"
        else:  # cls ==4
            cls_name = "ä¸¥é‡é£åˆ‡å˜"
        logger.info(f"ç±»åˆ«{cls}ï¼ˆ{cls_name}ï¼‰ï¼š{count:,} ä¸ªç‚¹ï¼ˆå æ¯”ï¼š{ratio:.2f}%ï¼‰")
    logger.info(f"è®­ç»ƒé›†æ€»ç‚¹æ•°ï¼š{total_points:,}")
    logger.info("=" * 60)

    return class_counts

def plot_loss_curve(epochs, train_loss, val_loss, save_dir):
    """ç»˜åˆ¶è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿å¹¶ä¿å­˜"""
    plt.figure(figsize=(10, 6))
    # ç»˜åˆ¶è®­ç»ƒæŸå¤± â†’ æ ‡ç­¾æ”¹ä¸º "Train Loss"
    plt.plot(epochs, train_loss, color='#e74c3c', linewidth=2.5, marker='o', markersize=4, label='Train Loss')
    # ç»˜åˆ¶éªŒè¯æŸå¤± â†’ æ ‡ç­¾æ”¹ä¸º "Val Loss"
    plt.plot(epochs, val_loss, color='#3498db', linewidth=2.5, marker='s', markersize=4, label='Val Loss')

    plt.title('Training and Validation Loss Curve', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xticks(np.arange(0, len(epochs) + 1, step=5))

    save_path = os.path.join(save_dir, 'loss_curve.png')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()


def plot_f1_curve(epochs, train_f1, val_f1, save_dir):
    """ç»˜åˆ¶è®­ç»ƒ/éªŒè¯F1æ›²çº¿å¹¶ä¿å­˜"""
    plt.figure(figsize=(10, 6))
    # ç»˜åˆ¶è®­ç»ƒF1 â†’ æ ‡ç­¾æ”¹ä¸º "Train F1"
    plt.plot(epochs, train_f1, color='#2ecc71', linewidth=2.5, marker='o', markersize=4, label='Train F1')
    # ç»˜åˆ¶éªŒè¯F1 â†’ æ ‡ç­¾æ”¹ä¸º "Val F1"
    plt.plot(epochs, val_f1, color='#f39c12', linewidth=2.5, marker='s', markersize=4, label='Val F1')

    plt.title('Training and Validation F1 Score Curve', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('F1 Score', fontsize=12)
    plt.ylim(0.5, 1.0)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.xticks(np.arange(0, len(epochs) + 1, step=5))

    save_path = os.path.join(save_dir, 'f1_curve.png')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def calculate_metrics_gpu(logits, labels, criterion, num_classes=5):
    """
    GPUå‘é‡åŒ–è®¡ç®—æŒ‡æ ‡ï¼ˆæ›¿ä»£sklearnï¼Œæ— CPUè½¬ç§»ï¼Œæ— å¾ªç¯ï¼‰
    è¿”å›ï¼šå½“å‰batchçš„æŸå¤±ã€TPã€FPã€FNã€æ€»ç‚¹æ•°
    """
    # 1. è®¡ç®—å½“å‰batchçš„æŸå¤±ï¼ˆå¸¦æƒé‡ï¼Œä¸è®­ç»ƒä¸€è‡´ï¼‰
    loss = criterion(logits, labels)  # criterionå·²åœ¨mainä¸­å®šä¹‰ï¼ˆå¸¦ç±»åˆ«æƒé‡ï¼‰

    # 2. é¢„æµ‹ç»“æœï¼ˆGPUä¸Šç›´æ¥è®¡ç®—ï¼Œä¸è½¬ç§»CPUï¼‰
    preds = torch.argmax(logits, dim=1)  # (N,)
    N = labels.shape[0]  # å½“å‰batchçš„æ€»ç‚¹æ•°

    # 3. å‘é‡åŒ–è®¡ç®—æ··æ·†çŸ©é˜µï¼ˆGPUä¸Šç”¨bincountï¼Œæ¯”sklearnå¿«100å€ï¼‰
    # åŸç†ï¼šç”¨ (labels * num_classes + preds) ç”Ÿæˆå”¯ä¸€ç´¢å¼•ï¼Œç»Ÿè®¡æ¯ä¸ªç´¢å¼•çš„æ•°é‡
    confusion = torch.bincount(
        labels * num_classes + preds,
        minlength=num_classes * num_classes
    ).view(num_classes, num_classes)  # (num_classes, num_classes)

    # 4. è®¡ç®—TPã€FPã€FNï¼ˆGPUå¼ é‡ï¼Œæ— éœ€å¾ªç¯ï¼‰
    tp = torch.diag(confusion)  # å¯¹è§’çº¿ä¸Šæ˜¯TPï¼ˆæ¯ä¸ªç±»çš„æ­£ç¡®é¢„æµ‹æ•°ï¼‰
    fp = confusion.sum(dim=1) - tp  # è¡Œå’Œ - TP = FPï¼ˆé¢„æµ‹å¯¹ä½†çœŸå®é”™ï¼‰
    fn = confusion.sum(dim=0) - tp  # åˆ—å’Œ - TP = FNï¼ˆçœŸå®å¯¹ä½†é¢„æµ‹é”™ï¼‰

    return {
        "loss": loss * N,  # ç´¯è®¡æŸå¤±ï¼ˆä¹˜ä»¥ç‚¹æ•°ï¼Œåç»­æ±‚å¹³å‡ï¼‰
        "tp": tp,  # æ¯ç±»TPï¼ˆGPUå¼ é‡ï¼‰
        "fp": fp,  # æ¯ç±»FPï¼ˆGPUå¼ é‡ï¼‰
        "fn": fn,  # æ¯ç±»FNï¼ˆGPUå¼ é‡ï¼‰
        "total_points": N  # å½“å‰batchæ€»ç‚¹æ•°
    }


def main(config_path):
    # -------------------------- 1. åŠ è½½é…ç½®æ–‡ä»¶ --------------------------
    with open(config_path, 'r') as f:
        cfg = yaml.safe_load(f)

    # åˆå§‹åŒ–æ—¥å¿—
    logger = get_logger('wind_shear_train', log_dir='./logs')
    logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    logger.debug(f"é…ç½®è¯¦æƒ…: {cfg}")

    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # -------------------------- 2. åˆå§‹åŒ–æ•°æ®é›†å’ŒDataLoader --------------------------
    # å…³é”®ï¼šç¡®ä¿è®­ç»ƒ/éªŒè¯ç”¨åŒä¸€ä¸ªcollate_fn
    train_loader = build_train_dataloader(cfg)
    val_loader = build_val_dataloader(cfg)

    # ã€è°ƒè¯•ã€‘éªŒè¯collate_fnæ˜¯å¦ç”Ÿæ•ˆ
    try:
        train_iter = iter(train_loader)
        first_batch = next(train_iter)
        if first_batch is not None:
            logger.info("\nè®­ç»ƒé›†ç¬¬ä¸€ä¸ªbatchå­—æ®µéªŒè¯ï¼š")
            for key in first_batch:
                if isinstance(first_batch[key], torch.Tensor):
                    logger.info(f"  {key}: shape {first_batch[key].shape}, dtype {first_batch[key].dtype}")
                else:
                    logger.info(f"  {key}: type {type(first_batch[key])}")
        else:
            logger.warning("ç¬¬ä¸€ä¸ªbatchä¸ºç©ºï¼Œå¯èƒ½æ‰€æœ‰æ ·æœ¬å‡è¢«è¿‡æ»¤")
    except Exception as e:
        logger.error(f"æ‰“å°ç¬¬ä¸€ä¸ªbatchå¤±è´¥: {e}")

    logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)}, éªŒè¯é›†æ ·æœ¬æ•°: {len(val_loader.dataset)}")

    # -------------------------- 3. ç»Ÿè®¡è®­ç»ƒé›†ç±»åˆ«æ•°é‡ + è®¡ç®—ç±»åˆ«æƒé‡ --------------------------
    # æ­¥éª¤1ï¼šç»Ÿè®¡è®­ç»ƒé›†å„ç±»åˆ«æ•°é‡ï¼ˆç‚¹çº§åˆ«ï¼‰
    #train_class_counts = count_train_classes(train_loader)
    #train_class_counts = {0: 201761, 1: 32251009, 2: 3509758, 3: 692463, 4: 1064945}
    #total_points = 37719936
    #num_classes = 5

    # 2. è®¡ç®—é€†é¢‘ç‡æƒé‡
    #inverse_weights = []
    #for cls in range(num_classes):
    #    n_c = train_class_counts[cls]
    #    w_c = total_points / (num_classes * n_c)  # æ ¸å¿ƒå…¬å¼
    #    inverse_weights.append(w_c)

    # 3. ï¼ˆå¯é€‰ï¼‰æƒé‡å½’ä¸€åŒ–ï¼ˆé¿å…æƒé‡è¿‡å¤§å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ï¼‰
    #max_weight = max(inverse_weights)
    #inverse_weights = [w / max_weight for w in inverse_weights]  # å½’ä¸€åˆ°0~1

    #ç›´æ¥ä½¿ç”¨ï¼ˆ1-3ï¼‰è®¡ç®—å¥½çš„ç»“æœ
    #inverse_weights = [1.0, 0.006256, 0.057486, 0.291367, 0.189457]
    #æƒé‡è®¾è®¡ä¸ä¸šåŠ¡ç›®æ ‡ï¼ˆé£åˆ‡å˜æ£€æµ‹çš„æ ¸å¿ƒä»»åŠ¡ï¼‰å¼ºç»‘å®š
    inverse_weights = [0.05, 0.15, 0.3, 0.6, 0.45]


    # 4. è½¬æ¢ä¸ºGPUå¼ é‡
    weight_tensor = torch.tensor(inverse_weights, dtype=torch.float32, device=device)
    criterion = torch.nn.CrossEntropyLoss(weight=weight_tensor)

    # è½¬æ¢ä¸ºtorchå¼ é‡å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    logger.info("\n" + "=" * 60)
    logger.info("æœ€ç»ˆç±»åˆ«æƒé‡")
    logger.info("=" * 60)
    for cls in range(5):
        if cls == 0:
            cls_name = "æ— é£åˆ‡å˜"
        elif cls == 1:
            cls_name = "è½»åº¦é£åˆ‡å˜"
        elif cls == 2:
            cls_name = "ä¸­åº¦é£åˆ‡å˜"
        elif cls == 3:
            cls_name = "å¼ºçƒˆé£åˆ‡å˜"
        else:
            cls_name = "ä¸¥é‡é£åˆ‡å˜"
        logger.info(
            f"ç±»åˆ«{cls}ï¼ˆ{cls_name}ï¼‰ï¼šæƒé‡={inverse_weights[cls]:.6f} ")
    logger.info("=" * 60)

    # -------------------------- 4. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ï¼ˆåº”ç”¨æƒé‡ï¼‰ --------------------------
    model = build_model(cfg['model']).to(device)
    logger.info(f"æ¨¡å‹ç±»å‹: {model.__class__.__name__}")

    # ä¼˜åŒ–å™¨ï¼ˆAdamWï¼Œå¸¦æƒé‡è¡°å‡ï¼‰
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg['train']['optimizer']['lr'],
        weight_decay=cfg['train']['optimizer']['weight_decay']
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg['train']['epochs']
    )

    # ğŸŒŸ 1. åˆå§‹åŒ–æ•°æ®è®°å½•åˆ—è¡¨ï¼ˆå­˜å‚¨æ¯ä¸ªepochçš„æŒ‡æ ‡ï¼‰
    train_losses = []  # è®­ç»ƒæŸå¤±
    train_f1s = []  # è®­ç»ƒF1
    val_losses = []  # éªŒè¯æŸå¤±
    val_f1s = []  # éªŒè¯F1
    epochs_list = []  # epochåºå·ï¼ˆç”¨äºxè½´ï¼‰

    # ğŸŒŸ 2. åˆ›å»ºå›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹ï¼ˆä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
    plot_save_dir = "./logs_photo/plots"
    os.makedirs(plot_save_dir, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºå¤šçº§ç›®å½•

    # -------------------------- 5. è®­ç»ƒå¾ªç¯ --------------------------
    best_val_f1 = 0.0
    for epoch in range(1, cfg['train']['epochs'] + 1):
        epochs_list.append(epoch)  # è®°å½•å½“å‰epoch
        logger.info(f"\n===== Epoch {epoch}/{cfg['train']['epochs']} =====")

        # -------------------------- 5.1 è®­ç»ƒé˜¶æ®µ --------------------------
        model.train()
        # åˆå§‹åŒ–GPUå¼ é‡ç”¨äºç´¯è®¡ï¼ˆæ›¿ä»£listï¼Œé¿å…CPUè½¬ç§»ï¼‰
        train_tp = torch.zeros(5, dtype=torch.long, device=device)  # æ¯ç±»TP
        train_fp = torch.zeros(5, dtype=torch.long, device=device)  # æ¯ç±»FP
        train_fn = torch.zeros(5, dtype=torch.long, device=device)  # æ¯ç±»FN
        train_total_loss = 0.0  # ç´¯è®¡æŸå¤±ï¼ˆå¸¦æƒé‡ï¼‰
        train_total_points = 0  # ç´¯è®¡æ€»ç‚¹æ•°
        abnormal_train_batches = []
        total_train_batches = 0
        normal_train_batches = 0

        for batch_idx, batch in enumerate(tqdm(train_loader, desc="è®­ç»ƒä¸­")):
            total_train_batches += 1
            if batch is None or len(batch['path']) == 0:
                logger.warning(f"è·³è¿‡ç©ºè®­ç»ƒbatch {batch_idx}")
                continue

            # è½¬ç§»batchåˆ°è®¾å¤‡ï¼ˆä¸å˜ï¼‰
            batch_device = {}
            for k, v in batch.items():
                if isinstance(v, torch.Tensor):
                    batch_device[k] = v.to(device)
                elif k == 'path':
                    batch_device[k] = v
            batch = batch_device
            labels = batch['generate_label'].long()  # (N,)
            logits = model(batch)  # (N, 5)

            # å¼‚å¸¸æ£€æµ‹ï¼ˆä¸å˜ï¼‰
            loss = criterion(logits, labels)
            if torch.isnan(loss) or torch.isinf(loss):
                sample_paths = [os.path.basename(p) for p in batch.get('path', ['æœªçŸ¥è·¯å¾„'])]
                abnormal_info = {"batch_idx": batch_idx, "sample_paths": sample_paths,
                                 "loss_value": loss.item() if not torch.isnan(loss) else "nan",
                                 "points_count": labels.shape[0]}
                abnormal_train_batches.append(abnormal_info)
                logger.error(
                    f"âŒ è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å¼‚å¸¸: loss={abnormal_info['loss_value']}, æ ·æœ¬è·¯å¾„={sample_paths}, ç‚¹æ•°={labels.shape[0]}")
                continue

            # æ­£å¸¸æ‰¹æ¬¡ï¼šåå‘ä¼ æ’­ï¼ˆä¸å˜ï¼‰
            normal_train_batches += 1
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
            optimizer.step()

            # ğŸŒŸ å…³é”®ä¿®æ”¹ï¼šä¼ å…¥ criterion åˆ° calculate_metrics_gpu
            metrics = calculate_metrics_gpu(logits, labels, criterion)
            train_total_loss += metrics["loss"].item()  # ç´¯è®¡æŸå¤±ï¼ˆè½¬ä¸ºfloaté¿å…æ˜¾å­˜å ç”¨ï¼‰
            train_tp += metrics["tp"]
            train_fp += metrics["fp"]
            train_fn += metrics["fn"]
            train_total_points += metrics["total_points"]

        # ğŸŒŸ è®­ç»ƒæŒ‡æ ‡è®¡ç®—ï¼ˆGPUä¸Šå®Œæˆï¼Œç§’çº§ï¼‰
        train_avg_loss = train_total_loss / train_total_points if train_total_points > 0 else 0.0
        # è®¡ç®—æ¯ç±»ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ï¼ˆé¿å…é™¤0ï¼‰
        epsilon = 1e-6
        train_precision = train_tp / (train_tp + train_fp + epsilon)  # (5,)
        train_recall = train_tp / (train_tp + train_fn + epsilon)  # (5,)
        train_f1 = 2 * (train_precision * train_recall) / (train_precision + train_recall + epsilon)  # (5,)
        # åŠ æƒå¹³å‡ï¼ˆæŒ‰æ¯ç±»æ ·æœ¬æ•°åŠ æƒï¼Œä¸sklearnçš„average='weighted'ä¸€è‡´ï¼‰
        class_counts = train_tp + train_fn  # æ¯ç±»çœŸå®æ ·æœ¬æ•°ï¼ˆTP+FNï¼‰
        total_counts = class_counts.sum()
        train_weighted_f1 = (train_f1 * class_counts).sum() / (total_counts + epsilon)
        train_weighted_precision = (train_precision * class_counts).sum() / (total_counts + epsilon)
        train_weighted_recall = (train_recall * class_counts).sum() / (total_counts + epsilon)

        # ğŸŒŸ è®°å½•è®­ç»ƒæŒ‡æ ‡
        train_losses.append(train_avg_loss if train_total_points > 0 else 0.0)
        train_f1s.append(train_weighted_f1.item() if train_total_points > 0 else 0.0)

        # æ—¥å¿—æ‰“å°ï¼ˆä¸å˜ï¼Œä»…å°†GPUå¼ é‡è½¬ä¸ºCPUæ•°å€¼ï¼‰
        if train_total_points > 0:
            logger.info(
                f"è®­ç»ƒé›†: æŸå¤±={train_avg_loss:.4f}, "
                f"å¬å›ç‡={train_weighted_recall.item():.4f}, ç²¾ç¡®ç‡={train_weighted_precision.item():.4f}, F1={train_weighted_f1.item():.4f}"
            )
        else:
            logger.warning("æœ¬epochæ— æœ‰æ•ˆè®­ç»ƒæ ·æœ¬ï¼Œè·³è¿‡è®­ç»ƒæŒ‡æ ‡è®¡ç®—")

        # -------------------------- 5.2 éªŒè¯é˜¶æ®µ --------------------------
        if epoch % cfg['evaluation']['interval'] == 0:
            model.eval()
            # åˆå§‹åŒ–GPUå¼ é‡ç´¯è®¡ï¼ˆæ›¿ä»£listï¼‰
            val_tp = torch.zeros(5, dtype=torch.long, device=device)
            val_fp = torch.zeros(5, dtype=torch.long, device=device)
            val_fn = torch.zeros(5, dtype=torch.long, device=device)
            val_total_loss = 0.0
            val_total_points = 0

            with torch.no_grad():
                for batch_idx, batch in enumerate(tqdm(val_loader, desc="éªŒè¯ä¸­")):
                    if batch is None:
                        logger.warning(f"è·³è¿‡ç©ºéªŒè¯batch {batch_idx}")
                        continue

                    # è½¬ç§»batchåˆ°è®¾å¤‡ï¼ˆä¸å˜ï¼‰
                    batch_device = {}
                    for k, v in batch.items():
                        if isinstance(v, torch.Tensor):
                            batch_device[k] = v.to(device)
                        elif k == 'path':
                            batch_device[k] = v
                    batch = batch_device
                    labels = batch['generate_label'].long()
                    logits = model(batch)

                    # ğŸŒŸ ä¼ å…¥ criterion åˆ° calculate_metrics_gpu
                    metrics = calculate_metrics_gpu(logits, labels, criterion)
                    val_total_loss += metrics["loss"].item()
                    val_tp += metrics["tp"]
                    val_fp += metrics["fp"]
                    val_fn += metrics["fn"]
                    val_total_points += metrics["total_points"]

                    # å¼‚å¸¸æ£€æµ‹ï¼ˆä¸å˜ï¼‰
                    if torch.isnan(metrics["loss"]) or torch.isinf(metrics["loss"]):
                        sample_paths = [os.path.basename(p) for p in batch.get('path', ['æœªçŸ¥è·¯å¾„'])]
                        logger.error(
                            f"âŒ éªŒè¯æ‰¹æ¬¡ {batch_idx} å¼‚å¸¸: loss={metrics['loss'].item() if not torch.isnan(metrics['loss']) else 'nan'}, æ ·æœ¬è·¯å¾„={sample_paths}, ç‚¹æ•°={labels.shape[0]}")

            # ğŸŒŸ éªŒè¯æŒ‡æ ‡è®¡ç®—ï¼ˆGPUä¸Šå®Œæˆï¼‰
            val_avg_loss = val_total_loss / val_total_points if val_total_points > 0 else 0.0
            val_precision = val_tp / (val_tp + val_fp + epsilon)
            val_recall = val_tp / (val_tp + val_fn + epsilon)
            val_f1 = 2 * (val_precision * val_recall) / (val_precision + val_recall + epsilon)
            val_class_counts = val_tp + val_fn
            val_total_counts = val_class_counts.sum()
            val_weighted_f1 = (val_f1 * val_class_counts).sum() / (val_total_counts + epsilon)
            val_weighted_precision = (val_precision * val_class_counts).sum() / (val_total_counts + epsilon)
            val_weighted_recall = (val_recall * val_class_counts).sum() / (val_total_counts + epsilon)

            # ğŸŒŸ è®°å½•éªŒè¯æŒ‡æ ‡
            val_losses.append(val_avg_loss if val_total_points > 0 else 0.0)
            val_f1s.append(val_weighted_f1.item() if val_total_points > 0 else 0.0)

            # æ—¥å¿—æ‰“å°+æœ€ä½³æ¨¡å‹ä¿å­˜ï¼ˆä¸å˜ï¼Œä»…æ›¿æ¢æŒ‡æ ‡å˜é‡ï¼‰
            if val_total_points > 0:
                logger.info(
                    f"éªŒè¯é›†: æŸå¤±={val_avg_loss:.4f}, "
                    f"å¬å›ç‡={val_weighted_recall.item():.4f}, ç²¾ç¡®ç‡={val_weighted_precision.item():.4f}, F1={val_weighted_f1.item():.4f}"
                )
                if val_weighted_f1.item() > best_val_f1:
                    best_val_f1 = val_weighted_f1.item()
                    save_checkpoint(model, optimizer, scheduler, epoch,
                                    save_path=f"./checkpoints/best_model_epoch{epoch}.pth")
                    logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (F1={best_val_f1:.4f}) åˆ° ./checkpoints/")
            else:
                logger.warning("æœ¬epochæ— æœ‰æ•ˆéªŒè¯æ ·æœ¬ï¼Œè·³è¿‡éªŒè¯æŒ‡æ ‡è®¡ç®—å’Œæ¨¡å‹ä¿å­˜")
        else:
            # ä¸æ‰§è¡ŒéªŒè¯æ—¶ï¼Œå‘ val_losses/val_f1s è¿½åŠ é»˜è®¤å€¼ï¼ˆä¿è¯åˆ—è¡¨é•¿åº¦ä¸€è‡´ï¼‰
            val_losses.append(0.0)
            val_f1s.append(0.0)

        # ğŸŒŸ 4. ç»˜åˆ¶å¹¶ä¿å­˜æ›²çº¿ï¼ˆæ¯ä¸ªepochæ›´æ–°ï¼‰
        if epoch % 1 == 0:
            plot_loss_curve(epochs_list, train_losses, val_losses, plot_save_dir)
            plot_f1_curve(epochs_list, train_f1s, val_f1s, plot_save_dir)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨æ­¥è¿›
        scheduler.step()

    logger.info(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›†F1åˆ†æ•°: {best_val_f1:.4f}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='configs/wind_shear/pointtransformer_v3.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    # é¢„å…ˆåˆ›å»ºæ—¥å¿—å’Œæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs('./logs', exist_ok=True)
    os.makedirs('./checkpoints', exist_ok=True)

    main(args.config)