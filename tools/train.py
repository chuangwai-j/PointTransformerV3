import os
import sys
import logging
# è§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜ï¼šå°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonæœç´¢è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import yaml
import torch
import numpy as np
from datetime import datetime
from tqdm import tqdm
from sklearn.metrics import recall_score, precision_score, f1_score
import warnings

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from pointcept.datasets.builder import build_train_dataloader, build_val_dataloader
from pointcept.models import build_model
from pointcept.utils.logger import get_logger
from pointcept.utils.checkpoint import save_checkpoint
from pointcept.utils.logging import setup_logging  # å¯¼å…¥å·¥å…·å‡½æ•°


# 1. é…ç½®å…¨å±€æ—¥å¿—ï¼ˆåªè°ƒç”¨1æ¬¡ï¼ï¼‰
logger = setup_logging(log_dir="./logs")  # æ—¥å¿—æ–‡ä»¶å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„logsæ–‡ä»¶å¤¹

def main(config_path):
    # -------------------------- 1. åŠ è½½é…ç½®æ–‡ä»¶ --------------------------
    with open(config_path, 'r') as f:
        cfg = yaml.safe_load(f)

    # åˆå§‹åŒ–æ—¥å¿—
    logger = get_logger('wind_shear_train', log_dir='./logs')
    logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
    logger.debug(f"é…ç½®è¯¦æƒ…: {cfg}")

    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # -------------------------- 2. åˆå§‹åŒ–æ•°æ®é›†å’ŒDataLoader --------------------------
    # å…³é”®ï¼šç¡®ä¿è®­ç»ƒ/éªŒè¯ç”¨åŒä¸€ä¸ªcollate_fnï¼ˆéœ€åœ¨build_train_dataloader/build_val_dataloaderä¸­æŒ‡å®šï¼‰
    train_loader = build_train_dataloader(cfg)
    val_loader = build_val_dataloader(cfg)

    # ã€è°ƒè¯•ã€‘éªŒè¯collate_fnæ˜¯å¦ç”Ÿæ•ˆ
    try:
        train_iter = iter(train_loader)
        first_batch = next(train_iter)
        if first_batch is None:
            logger.warning("ç¬¬ä¸€ä¸ªbatchä¸ºç©ºï¼Œå¯èƒ½æ‰€æœ‰æ ·æœ¬å‡è¢«è¿‡æ»¤")
            for key in first_batch:
                if isinstance(first_batch[key], torch.Tensor):
                    logger.info(f"  {key}: shape {first_batch[key].shape}, dtype {first_batch[key].dtype}")
                else:
                    logger.info(f"  {key}: type {type(first_batch[key])}")
    except Exception as e:
        logger.error(f"æ‰“å°ç¬¬ä¸€ä¸ªbatchå¤±è´¥: {e}")

    logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_loader.dataset)}, éªŒè¯é›†æ ·æœ¬æ•°: {len(val_loader.dataset)}")

    # -------------------------- 3. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•° --------------------------
    model = build_model(cfg['model']).to(device)
    logger.info(f"æ¨¡å‹ç±»å‹: {model.__class__.__name__}")

    # ä¼˜åŒ–å™¨ï¼ˆAdamWï¼Œå¸¦æƒé‡è¡°å‡ï¼‰
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg['train']['optimizer']['lr'],
        weight_decay=cfg['train']['optimizer']['weight_decay']
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆä½™å¼¦é€€ç«ï¼‰
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg['train']['epochs']
    )

    # å¤šåˆ†ç±»æŸå¤±å‡½æ•°ï¼ˆCrossEntropyLossé€‚ç”¨äºç±»åˆ«äº’æ–¥çš„å¤šåˆ†ç±»ï¼‰
    criterion = torch.nn.CrossEntropyLoss()

    # -------------------------- 4. è®­ç»ƒå¾ªç¯ --------------------------
    best_val_f1 = 0.0
    for epoch in range(1, cfg['train']['epochs'] + 1):
        logger.info(f"\n===== Epoch {epoch}/{cfg['train']['epochs']} =====")

        # -------------------------- 4.1 è®­ç»ƒé˜¶æ®µ --------------------------
        model.train()
        train_loss = 0.0
        train_preds = []
        train_labels = []
        total_train_points = 0  # ç”¨äºè®¡ç®—å¹³å‡æŸå¤±çš„å®é™…æ€»ç‚¹æ•°
        # æ–°å¢ï¼šå¼‚å¸¸æ‰¹æ¬¡ç»Ÿè®¡å˜é‡
        abnormal_train_batches = []  # è®°å½•å¼‚å¸¸æ‰¹æ¬¡ä¿¡æ¯
        total_train_batches = 0  # æ€»è®­ç»ƒæ‰¹æ¬¡
        normal_train_batches = 0  # æ­£å¸¸è®­ç»ƒæ‰¹æ¬¡

        for batch_idx, batch in enumerate(tqdm(train_loader, desc="è®­ç»ƒä¸­")):
            # ğŸŒŸ å…³é”®ï¼šæ‰“å°ä¼ å…¥æ¨¡å‹å‰çš„ batch å­—æ®µ
            #print(f"train.py ä¸­ batch çš„å­—æ®µï¼š{list(batch.keys())}")  # å¿…é¡»åŠ è¿™è¡Œï¼
            #print(f"train.py ä¸­ batch çš„ pathï¼š{batch.get('path', 'æ— ')}")  # æŸ¥çœ‹ path æ˜¯å¦å­˜åœ¨
            total_train_batches += 1  # ç´¯è®¡æ€»æ‰¹æ¬¡
            # å…³é”®ä¿®æ”¹ï¼šè·³è¿‡ç©ºbatch
            if batch is None or len(batch['path']) == 0:
                logger.warning(f"è·³è¿‡ç©ºè®­ç»ƒbatch {batch_idx}ï¼ˆæ— æœ‰æ•ˆæ ·æœ¬ï¼‰")
                continue

            # è½¬ç§»batchåˆ°è®¾å¤‡
            #batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            # ğŸŒŸ ä¿®æ”¹åï¼šä¿ç•™pathï¼ŒåŒæ—¶å°†å¼ é‡è½¬ç§»åˆ°è®¾å¤‡
            batch_device = {}
            # å…ˆå¤„ç†å¼ é‡å­—æ®µï¼ˆè½¬ç§»åˆ°è®¾å¤‡ï¼‰
            for k, v in batch.items():
                if isinstance(v, torch.Tensor):
                    batch_device[k] = v.to(device)
                # æ˜¾å¼ä¿ç•™pathï¼ˆéå¼ é‡ï¼‰
                elif k == 'path':
                    batch_device[k] = v
            # ç”¨å¤„ç†åçš„batchæ›¿ä»£åŸbatch
            batch = batch_device
            labels = batch['generate_label'].long()  # å¤šåˆ†ç±»æ ‡ç­¾éœ€ä¸ºé•¿æ•´æ•°ç±»å‹
            current_points = batch['coord'].size(0)  # å½“å‰batchçš„å®é™…ç‚¹æ•°

            # å‰å‘ä¼ æ’­+åå‘ä¼ æ’­
            optimizer.zero_grad()
            logits = model(batch)  # å…³é”®ï¼šç°åœ¨outputsç›´æ¥æ˜¯logits tensor
            loss = criterion(logits, labels)

            # å¼‚å¸¸æ£€æµ‹
            if torch.isnan(loss) or torch.isinf(loss):
                sample_paths = [os.path.basename(p) for p in batch.get('path', ['æœªçŸ¥è·¯å¾„'])]
                abnormal_info = {
                    "batch_idx": batch_idx,
                    "sample_paths": sample_paths,
                    "loss_value": loss.item() if not torch.isnan(loss) else "nan",
                    "points_count": current_points
                }
                abnormal_train_batches.append(abnormal_info)
                logger.error(
                    f"âŒ è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å¼‚å¸¸: loss={abnormal_info['loss_value']}, æ ·æœ¬è·¯å¾„={sample_paths}, ç‚¹æ•°={current_points}")
                continue

            # æ­£å¸¸æ‰¹æ¬¡ï¼šæ›´æ–°å‚æ•°+ç´¯è®¡æŒ‡æ ‡
            normal_train_batches += 1
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) # æ¢¯åº¦è£å‰ª
            optimizer.step()

            train_loss += loss.item() * current_points
            total_train_points += current_points
            train_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())  # å¤šåˆ†ç±»å–argmax
            train_labels.extend(labels.cpu().numpy())

        # è®­ç»ƒé˜¶æ®µæ±‡æ€»
        logger.info("\n===== è®­ç»ƒé˜¶æ®µæ‰¹æ¬¡ç»Ÿè®¡ =====")
        logger.info(
            f"æ€»æ‰¹æ¬¡: {total_train_batches}, æ­£å¸¸æ‰¹æ¬¡: {normal_train_batches}, å¼‚å¸¸æ‰¹æ¬¡: {len(abnormal_train_batches)}")
        if abnormal_train_batches:
            logger.error(f"å¼‚å¸¸æ‰¹æ¬¡ç´¢å¼•åˆ—è¡¨: {[info['batch_idx'] for info in abnormal_train_batches]}")
            logger.error(f"é¦–ä¸ªå¼‚å¸¸æ‰¹æ¬¡è¯¦æƒ…: {abnormal_train_batches[0]}")
        else:
            logger.info("âœ… æ‰€æœ‰è®­ç»ƒæ‰¹æ¬¡å‡æ­£å¸¸")

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        if total_train_points > 0:
            train_loss /= total_train_points
            train_recall = recall_score(train_labels, train_preds, average='weighted', zero_division=0)
            train_precision = precision_score(train_labels, train_preds, average='weighted', zero_division=0)
            train_f1 = f1_score(train_labels, train_preds, average='weighted', zero_division=0)
            logger.info(
                f"è®­ç»ƒé›†: æŸå¤±={train_loss:.4f}, "
                f"å¬å›ç‡={train_recall:.4f}, ç²¾ç¡®ç‡={train_precision:.4f}, F1={train_f1:.4f}"
            )
        else:
            logger.warning("æœ¬epochæ— æœ‰æ•ˆè®­ç»ƒæ ·æœ¬ï¼Œè·³è¿‡è®­ç»ƒæŒ‡æ ‡è®¡ç®—")

        # -------------------------- 4.2 éªŒè¯é˜¶æ®µï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰ --------------------------
        if epoch % cfg['evaluation']['interval'] == 0:
            model.eval()
            val_loss = 0.0
            val_preds = []
            val_labels = []
            total_val_points = 0
            first_val_batch = True

            with torch.no_grad():   # å…³é—­æ¢¯åº¦ï¼Œä¸å½±å“æ•°å€¼è®¡ç®—
                for batch_idx, batch in enumerate(tqdm(val_loader, desc="éªŒè¯ä¸­")):
                    # å…³é”®ä¿®æ”¹ï¼šè·³è¿‡ç©ºbatch
                    if batch is None:
                        logger.warning(f"è·³è¿‡ç©ºéªŒè¯batch {batch_idx}ï¼ˆæ— æœ‰æ•ˆæ ·æœ¬ï¼‰")
                        continue

                    # 1. è½¬ç§»è®¾å¤‡+åŸºç¡€ä¿¡æ¯
                    #batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
                    # ğŸŒŸ ä¿®æ”¹åï¼šä¿ç•™pathï¼ŒåŒæ—¶å°†å¼ é‡è½¬ç§»åˆ°è®¾å¤‡
                    batch_device = {}
                    for k, v in batch.items():
                        if isinstance(v, torch.Tensor):
                            batch_device[k] = v.to(device)
                        elif k == 'path':
                            batch_device[k] = v
                    batch = batch_device
                    labels = batch['generate_label'].long()  # å¤šåˆ†ç±»æ ‡ç­¾éœ€ä¸ºé•¿æ•´æ•°ç±»å‹
                    current_points = batch['coord'].size(0)
                    sample_paths = [os.path.basename(p) for p in batch.get('path', ['æœªçŸ¥è·¯å¾„'])]

                    # 2. åªè°ƒç”¨ä¸€æ¬¡æ¨¡å‹å‰å‘ï¼ˆä¿®å¤é‡å¤è°ƒç”¨bugï¼‰
                    logits = model(batch)
                    '''
                    # 3. ç¬¬ä¸€ä¸ªéªŒè¯batchï¼šå¢å¼ºè°ƒè¯•æ—¥å¿—ï¼ˆå¯¹æ¯”è®­ç»ƒé›†ï¼‰
                    if first_val_batch:
                        logger.info("\n=== éªŒè¯é›†ç¬¬ä¸€ä¸ªbatchå…³é”®ä¿¡æ¯ï¼ˆä¸è®­ç»ƒé›†å¯¹æ¯”ï¼‰ ===")
                        # æ‰“å°logitsç»Ÿè®¡
                        logger.info(
                            f"logitså½¢çŠ¶: {logits.shape}, æœ€å°å€¼: {logits.min().item():.4f}, æœ€å¤§å€¼: {logits.max().item():.4f}")
                        logger.info(
                            f"logitså«nan: {torch.isnan(logits).any().item()}, å«inf: {torch.isinf(logits).any().item()}")
                        # æ‰“å°coordèŒƒå›´ï¼ˆå…³é”®ï¼šå¯¹æ¯”è®­ç»ƒé›†æ˜¯å¦ä¸€è‡´ï¼‰
                        coord_min = batch['coord'].min(axis=0).values
                        coord_max = batch['coord'].max(axis=0).values
                        logger.info(
                            f"coordèŒƒå›´: x[{coord_min[0]:.0f}~{coord_max[0]:.0f}], y[{coord_min[1]:.0f}~{coord_max[1]:.0f}], z[{coord_min[2]:.0f}~{coord_max[2]:.0f}]")
                        # æ‰“å°spatial_shapeï¼ˆä¸´æ—¶è®¡ç®—ï¼šcoord_max+1ï¼Œåç»­æ¨¡å‹è¿”å›åå¯æ›¿æ¢ï¼‰
                        spatial_shape = [int(coord_max[2].item()) + 1, int(coord_max[1].item()) + 1,
                                         int(coord_max[0].item()) + 1]  # z/y/x
                        logger.info(f"ä¸´æ—¶è®¡ç®—spatial_shape: {spatial_shape}ï¼ˆè‹¥æŸç»´åº¦>2000ï¼Œéœ€é™åˆ¶coordèŒƒå›´ï¼‰")
                        logger.info(f"æ ‡ç­¾èŒƒå›´: {labels.min().item()}~{labels.max().item()}, æ ·æœ¬è·¯å¾„: {sample_paths}")
                        first_val_batch = False
                    '''
                    # 4. ç´¯è®¡éªŒè¯æŒ‡æ ‡ï¼ˆä¿®å¤æœªç´¯è®¡bugï¼‰
                    loss = criterion(logits, labels)  # æ— å¤šä½™squeeze()
                    val_loss += loss.item() * current_points
                    total_val_points += current_points
                    val_preds.extend(torch.argmax(logits, dim=1).cpu().numpy())
                    val_labels.extend(labels.cpu().numpy())

                    # 5. éªŒè¯æ‰¹æ¬¡å¼‚å¸¸æ£€æµ‹
                    if torch.isnan(loss) or torch.isinf(loss):
                        logger.error(
                            f"âŒ éªŒè¯æ‰¹æ¬¡ {batch_idx} å¼‚å¸¸: loss={loss.item() if not torch.isnan(loss) else 'nan'}, æ ·æœ¬è·¯å¾„={sample_paths}, ç‚¹æ•°={current_points}")

            # è®¡ç®—éªŒè¯æŒ‡æ ‡
            logger.info("\n===== éªŒè¯é˜¶æ®µæ±‡æ€» =====")
            if total_val_points > 0:
                val_loss /= total_val_points
                val_recall = recall_score(val_labels, val_preds, average='weighted', zero_division=0)
                val_precision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)
                val_f1 = f1_score(val_labels, val_preds, average='weighted', zero_division=0)
                logger.info(
                    f"éªŒè¯é›†: æŸå¤±={val_loss:.4f}, "
                    f"å¬å›ç‡={val_recall:.4f}, ç²¾ç¡®ç‡={val_precision:.4f}, F1={val_f1:.4f}"
                )

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_f1 > best_val_f1:
                    best_val_f1 = val_f1
                    save_checkpoint(
                        model, optimizer, scheduler, epoch,
                        save_path=f"./checkpoints/best_model_epoch{epoch}.pth"
                    )
                    logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (F1={best_val_f1:.4f}) åˆ° ./checkpoints/")
            else:
                logger.warning("æœ¬epochæ— æœ‰æ•ˆéªŒè¯æ ·æœ¬ï¼Œè·³è¿‡éªŒè¯æŒ‡æ ‡è®¡ç®—å’Œæ¨¡å‹ä¿å­˜")

        # å­¦ä¹ ç‡è°ƒåº¦å™¨æ­¥è¿›
        scheduler.step()

    logger.info(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯é›†F1åˆ†æ•°: {best_val_f1:.4f}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='configs/wind_shear/pointtransformer_v3.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    # é¢„å…ˆåˆ›å»ºæ—¥å¿—å’Œæ£€æŸ¥ç‚¹ç›®å½•
    os.makedirs('./logs', exist_ok=True)
    os.makedirs('./checkpoints', exist_ok=True)

    main(args.config)
